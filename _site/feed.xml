<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:8080/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:8080/" rel="alternate" type="text/html" /><updated>2023-11-07T00:00:20+00:00</updated><id>http://localhost:8080/feed.xml</id><title type="html">LLAMABOX</title><subtitle>Viaggio nel mondo dell&apos;addestramento dei modelli di intelligenza artificiale</subtitle><entry><title type="html">Welcome to Llamabox!</title><link href="http://localhost:8080/jekyll/update/2023/11/06/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Llamabox!" /><published>2023-11-06T17:02:16+00:00</published><updated>2023-11-06T17:02:16+00:00</updated><id>http://localhost:8080/jekyll/update/2023/11/06/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:8080/jekyll/update/2023/11/06/welcome-to-jekyll.html"><![CDATA[<p>L’addestramento di modelli di lingua di grandi dimensioni come LLaMa 2.0 o 
ChatGPT-3.5 da zero può essere un’impresa fuori portata per la maggiorparte di noi.</p>

<p><strong>llama</strong> 
Un rapporto ha suggerito che il costo per addestrare vari modelli 
LLaMa (includendo le versioni da 7B, 13B, 33B, e 65B parametri) era di circa $7milioni.</p>

<p><strong>gpt3</strong> 
L’articolo suggerisce anche che l’addestramento di un modello GPT 
con 500 miliardi di parametri su un cluster CS-2 a 
quattro nodi potrebbe richiedere circa un anno, 
con un costo associato di diversi milioni di dollari, se non di più​.</p>

<p>Dobbiamo quindi rassegnarci a fare fine-tuning su modelli altrui? Forse non è poi così male, ma l’AI hardcore non è una cosa per tutti, almeno per ora.
<a href="https://www.nextplatform.com/2022/12/01/counting-the-cost-of-training-large-language-models/">Questo rapporto</a> fornisce altre utili informazioni su questo argomento.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[L’addestramento di modelli di lingua di grandi dimensioni come LLaMa 2.0 o ChatGPT-3.5 da zero può essere un’impresa fuori portata per la maggiorparte di noi.]]></summary></entry></feed>